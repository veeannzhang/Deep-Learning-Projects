{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2 Part II_Yingxin Zhang_yz3202.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Ji2cIf-kzATA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part II Run experiments using tf.keras model subclassing and eager"
      ]
    },
    {
      "metadata": {
        "id": "RcSMRHA7OTnT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set up"
      ]
    },
    {
      "metadata": {
        "id": "uR-5nIoq1NCv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U tensorboardcolab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f3qfi4PT0OLE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorboardcolab import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YXQU2YNn1UFy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tbc=TensorBoardColab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1M2ciAUtFYYS"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "tfe = tf.contrib.eager\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "skQBAMSgFgJF"
      },
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "# Dataset will be cached locally after you run this code\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# These types are required for the operation we use to compute\n",
        "# loss. Omit, and you shall receive a cryptic error message.\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_phAjMlEFsA0"
      },
      "cell_type": "code",
      "source": [
        "buffer_size = 5000\n",
        "batch_size = 100\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size)\n",
        "train_dataset = train_dataset.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TBOQC-VPK9nZ"
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oi0611F5GFyK"
      },
      "cell_type": "code",
      "source": [
        "def loss(logits, labels):\n",
        "  # FIX ME\n",
        "  # You will need to modify this function, of course.\n",
        "  # Best bet, use tf.nn.sparse_softmax_cross_entropy_with_logits\n",
        "  # though if you're interested, you can write your own.\n",
        "  return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = labels, logits = logits))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2vaut5jVGuwP"
      },
      "cell_type": "code",
      "source": [
        "def compute_accuracy(logits, labels):\n",
        "  # You shoud not need to modify this function\n",
        "  predictions = tf.argmax(logits, axis=1)\n",
        "  batch_size = int(logits.shape[0])\n",
        "  return tf.reduce_sum(\n",
        "      tf.cast(tf.equal(predictions, labels), dtype=tf.float32)) / batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wxM3LBNRGLdS"
      },
      "cell_type": "code",
      "source": [
        "def train(model, images, labels):\n",
        "  # You should not need to modify this function\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(images)\n",
        "    loss_value = loss(logits, labels)  \n",
        "  grads = tape.gradient(loss_value, model.variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.variables))\n",
        "  return loss_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tefjsq_GqXJP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. Complete the linear model sketched in your starter code. Your finished model should be >80% accurate."
      ]
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "acHRNledFwXL"
      },
      "cell_type": "code",
      "source": [
        "class Model(tf.keras.Model): # Linear Model\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    # FIX ME\n",
        "    # add some layers to your model\n",
        "    self.dense = tf.keras.layers.Dense(10)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    # FIX ME\n",
        "    # use your layers (don't forget to add activation functions here as well\n",
        "    # if you haven't specified them in your layer definintions)\n",
        "    x = self.dense(x)\n",
        "    return x # be sure to return logits, not softmax output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FncXV6IzG-In"
      },
      "cell_type": "code",
      "source": [
        "# The first time you run the below block it will crash\n",
        "# with an error 'ValueError: No variables provided.''\n",
        "# This is because the call method of your model\n",
        "# is not using any trainable variables.\n",
        "# (As written, the model just flattens the images.)\n",
        "\n",
        "model = Model()\n",
        "\n",
        "epochs = 10\n",
        "step_counter = 0\n",
        "my_loss = []\n",
        "\n",
        "for epoch_n in range(epochs):\n",
        "    print('Epoch #%d' % (epoch_n))\n",
        "    for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "        loss_value = train(model, images, labels)\n",
        "        my_loss.append(loss_value)\n",
        "        step_counter +=1\n",
        "  \n",
        "        if step_counter % 100 == 0:\n",
        "          print('Step #%d\\tLoss: %.4f' % (step_counter, loss_value))\n",
        "\n",
        "    test_accuracy = compute_accuracy(model(x_test), y_test)\n",
        "    print('Accuracy #%.2f\\n' % (test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LdaLYe6GILMR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. Visualizating Loss for Linear Model with TensorBoard"
      ]
    },
    {
      "metadata": {
        "id": "WgwKGJZwzyhk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "writer = tf.contrib.summary.create_file_writer(logdir='./Graph', flush_millis=1000)\n",
        "with writer.as_default():\n",
        "    with tf.contrib.summary.always_record_summaries():\n",
        "        for i in range(len(my_loss)):\n",
        "            tf.contrib.summary.scalar(\"loss_linear_model\", my_loss[i], step=i)\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UkG5WB2oP0Gh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vq6yNRRER12W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rA1Qvf6dSAUF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Image('loss_Linear_model.png',width=1200, height=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W43vz8K4Ba4X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5. Deep Model with Subclassing and Loss Visualization with Tensorboard\n"
      ]
    },
    {
      "metadata": {
        "id": "uAzus5Vc_Yhg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Deep_Model(tf.keras.Model): # Deep Model\n",
        "  def __init__(self):\n",
        "    super(Deep_Model, self).__init__()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    # FIX ME\n",
        "    # add some layers to your model\n",
        "    self.dense1 = tf.keras.layers.Dense(512)\n",
        "    self.drop1 = tf.keras.layers.Dropout(rate = 0.5)\n",
        "    self.dense2 = tf.keras.layers.Dense(512)\n",
        "    self.drop2 = tf.keras.layers.Dropout(rate = 0.5)\n",
        "    self.dense3 = tf.keras.layers.Dense(10)\n",
        "    \n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    # FIX ME\n",
        "    # use your layers (don't forget to add activation functions here as well\n",
        "    # if you haven't specified them in your layer definintions)\n",
        "    x = self.dense1(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.drop1(x)\n",
        "    x = self.dense2(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.drop2(x)\n",
        "    x = self.dense3(x)\n",
        "    return x # be sure to return logits, not softmax output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ysKZ2vCh_8th",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "deep_model = Deep_Model()\n",
        "\n",
        "epochs = 10\n",
        "step_counter = 0\n",
        "my_loss_deep = []\n",
        "\n",
        "for epoch_n in range(epochs):\n",
        "    print('Epoch #%d' % (epoch_n))\n",
        "    for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "         loss_value = train(deep_model, images, labels)\n",
        "         my_loss_deep.append(loss_value)\n",
        "         step_counter +=1\n",
        "  \n",
        "         if step_counter % 100 == 0:\n",
        "            print('Step #%d\\tLoss: %.4f' % (step_counter, loss_value))\n",
        "\n",
        "    test_accuracy = compute_accuracy(deep_model(x_test), y_test)\n",
        "    print('Accuracy #%.2f\\n' % (test_accuracy))\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LPKf1o24TFXF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Visualizing loss for Deep Model"
      ]
    },
    {
      "metadata": {
        "id": "VDXP4PzMABNQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "writer = tf.contrib.summary.create_file_writer(logdir='./Graph', flush_millis=1000)\n",
        "with writer.as_default():\n",
        "    with tf.contrib.summary.always_record_summaries():\n",
        "        for i in range(len(my_loss_deep)):\n",
        "            tf.contrib.summary.scalar(\"loss_deep_model\", my_loss_deep[i], step=i)\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BON0Aculjz4h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H1oXD7OOkReX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ajKKwzjkWji",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Image('loss_Deep_model.png',width=1200, height=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YVxTu0EumDXh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6. Design and run experients to compare"
      ]
    },
    {
      "metadata": {
        "id": "2YRdizvR8ylM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### a. High, low, and reasonable learning rate"
      ]
    },
    {
      "metadata": {
        "id": "V3-Z4CZ7DSaT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**High Learning Rate 0.5**"
      ]
    },
    {
      "metadata": {
        "id": "yOH8ooPTDc5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer_high = tf.train.GradientDescentOptimizer(learning_rate=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BTTyLKlAEOi6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_high(model, images, labels):\n",
        "  # You should not need to modify this function\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(images)\n",
        "    loss_value = loss(logits, labels)  \n",
        "  grads = tape.gradient(loss_value, model.variables)\n",
        "  optimizer_high.apply_gradients(zip(grads, model.variables))\n",
        "  return loss_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-cL4PuMYmKa-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "deep_model = Deep_Model()\n",
        "\n",
        "epochs = 10\n",
        "step_counter = 0\n",
        "my_loss_deep = []\n",
        "\n",
        "for epoch_n in range(epochs):\n",
        "    print('Epoch #%d' % (epoch_n))\n",
        "    for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "         loss_value = train_high(deep_model, images, labels)\n",
        "         my_loss_deep.append(loss_value)\n",
        "         step_counter +=1\n",
        "  \n",
        "         if step_counter % 100 == 0:\n",
        "            print('Step #%d\\tLoss: %.4f' % (step_counter, loss_value))\n",
        "\n",
        "    test_accuracy = compute_accuracy(deep_model(x_test), y_test)\n",
        "    print('Accuracy #%.2f\\n' % (test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gnaaTX4aF9aE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Visualizing Loss for Deep Model with High Learning Rate**"
      ]
    },
    {
      "metadata": {
        "id": "Qg-6rdInLSqL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "writer = tf.contrib.summary.create_file_writer(logdir='./Graph', flush_millis=1000)\n",
        "with writer.as_default():\n",
        "    with tf.contrib.summary.always_record_summaries():\n",
        "        for i in range(len(my_loss_deep)):\n",
        "            tf.contrib.summary.scalar(\"loss_deep_model_with_high_rate\", my_loss_deep[i], step=i)\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2u__OenrH7ma",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dPQRQX-NIJY8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Image('loss_deep_model_with_high_rate.png',width=1200, height=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aH9yvhLsFCys",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Low Learning Rate 0.01**"
      ]
    },
    {
      "metadata": {
        "id": "PPwJGlj2FHGz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "zhoptimizer_low = tf.train.GradientDescentOptimizer(learning_rate=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5sSnhVucFNfv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_low(model, images, labels):\n",
        "  # You should not need to modify this function\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(images)\n",
        "    loss_value = loss(logits, labels)  \n",
        "  grads = tape.gradient(loss_value, model.variables)\n",
        "  optimizer_low.apply_gradients(zip(grads, model.variables))\n",
        "  return loss_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QLYXNY_hFSqV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "deep_model = Deep_Model()\n",
        "\n",
        "epochs = 10\n",
        "step_counter = 0\n",
        "my_loss_deep = []\n",
        "\n",
        "for epoch_n in range(epochs):\n",
        "    print('Epoch #%d' % (epoch_n))\n",
        "    for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "         loss_value = train_low(deep_model, images, labels)\n",
        "         my_loss_deep.append(loss_value)\n",
        "         step_counter +=1\n",
        "  \n",
        "         if step_counter % 100 == 0:\n",
        "            print('Step #%d\\tLoss: %.4f' % (step_counter, loss_value))\n",
        "\n",
        "    test_accuracy = compute_accuracy(deep_model(x_test), y_test)\n",
        "    print('Accuracy #%.2f\\n' % (test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pg56Y1qAKve_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Visualizing Loss for Deep Model with Low Learning Rate**"
      ]
    },
    {
      "metadata": {
        "id": "jdmYc3tHKo5U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "writer = tf.contrib.summary.create_file_writer(logdir='./Graph', flush_millis=1000)\n",
        "with writer.as_default():\n",
        "    with tf.contrib.summary.always_record_summaries():\n",
        "        for i in range(len(my_loss_deep)):\n",
        "            tf.contrib.summary.scalar(\"loss_deep_model_with_low_rate\", my_loss_deep[i], step=i)\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sVZ0MB3NK7Il",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "from io import BytesIO\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yz_nKDncLYL1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Image('loss_deep_model_with_low_rate.png',width=1200, height=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xFX3Z9GoLkPK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Reasonable Learning Rate 0.2**"
      ]
    },
    {
      "metadata": {
        "id": "Zptn1d1iLjqs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer_reasonable = tf.train.GradientDescentOptimizer(learning_rate=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8P32p6cXMFs9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_reasonable(model, images, labels):\n",
        "  # You should not need to modify this function\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(images)\n",
        "    loss_value = loss(logits, labels)  \n",
        "  grads = tape.gradient(loss_value, model.variables)\n",
        "  optimizer_reasonable.apply_gradients(zip(grads, model.variables))\n",
        "  return loss_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kGJjPHogMLIX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "deep_model = Deep_Model()\n",
        "\n",
        "epochs = 10\n",
        "step_counter = 0\n",
        "my_loss_deep = []\n",
        "\n",
        "for epoch_n in range(epochs):\n",
        "    print('Epoch #%d' % (epoch_n))\n",
        "    for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "         loss_value = train_reasonable(deep_model, images, labels)\n",
        "         my_loss_deep.append(loss_value)\n",
        "         step_counter +=1\n",
        "  \n",
        "         if step_counter % 100 == 0:\n",
        "            print('Step #%d\\tLoss: %.4f' % (step_counter, loss_value))\n",
        "\n",
        "    test_accuracy = compute_accuracy(deep_model(x_test), y_test)\n",
        "    print('Accuracy #%.2f\\n' % (test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YVEQW90zMWHV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Visualizing Loss for Deep Model with Reasonable Learning Rate**"
      ]
    },
    {
      "metadata": {
        "id": "5SWDJ8AKMcmD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "writer = tf.contrib.summary.create_file_writer(logdir='./Graph', flush_millis=1000)\n",
        "with writer.as_default():\n",
        "    with tf.contrib.summary.always_record_summaries():\n",
        "        for i in range(len(my_loss_deep)):\n",
        "            tf.contrib.summary.scalar(\"loss_deep_model_with_reasonable_rate\", my_loss_deep[i], step=i)\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eqrAtxmAO4mV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "from io import BytesIO\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SsT1kHvUPMTN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Image('loss_deep_model_with_reasonable_rate.png',width=1200, height=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SGSc8quf87bZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### b. Different activation functions"
      ]
    },
    {
      "metadata": {
        "id": "T_gXCGflRL-y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Sigmoid Activation Function**\n"
      ]
    },
    {
      "metadata": {
        "id": "lcHCTwPyFMeE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Deep_Model_sigmoid(tf.keras.Model): # Deep Model\n",
        "  def __init__(self):\n",
        "    super(Deep_Model_sigmoid, self).__init__()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    # FIX ME\n",
        "    # add some layers to your model\n",
        "    self.dense1 = tf.keras.layers.Dense(512)\n",
        "    self.drop1 = tf.keras.layers.Dropout(rate = 0.5)\n",
        "    self.dense2 = tf.keras.layers.Dense(512)\n",
        "    self.drop2 = tf.keras.layers.Dropout(rate = 0.5)\n",
        "    self.dense3 = tf.keras.layers.Dense(10)\n",
        "    \n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    # FIX ME\n",
        "    # use your layers (don't forget to add activation functions here as well\n",
        "    # if you haven't specified them in your layer definintions)\n",
        "    x = self.dense1(x)\n",
        "    x = tf.nn.sigmoid(x)\n",
        "    x = self.drop1(x)\n",
        "    x = self.dense2(x)\n",
        "    x = tf.nn.sigmoid(x)\n",
        "    x = self.drop2(x)\n",
        "    x = self.dense3(x)\n",
        "    return x # be sure to return logits, not softmax output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UOFLUygG9CbN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "deep_model_sigmoid = Deep_Model_sigmoid()\n",
        "\n",
        "epochs = 10\n",
        "step_counter = 0\n",
        "my_loss_deep = []\n",
        "\n",
        "for epoch_n in range(epochs):\n",
        "    print('Epoch #%d' % (epoch_n))\n",
        "    for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "         loss_value = train(deep_model_sigmoid, images, labels)\n",
        "         my_loss_deep.append(loss_value)\n",
        "         step_counter +=1\n",
        "  \n",
        "         if step_counter % 100 == 0:\n",
        "            print('Step #%d\\tLoss: %.4f' % (step_counter, loss_value))\n",
        "\n",
        "    test_accuracy = compute_accuracy(deep_model_sigmoid(x_test), y_test)\n",
        "    print('Accuracy #%.2f\\n' % (test_accuracy))\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wRwPZoluYXDB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Visualize Loss for Deep Model with Sigmoid Activation Function**"
      ]
    },
    {
      "metadata": {
        "id": "wIp_KHbZYTS7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "writer = tf.contrib.summary.create_file_writer(logdir='./Graph', flush_millis=1000)\n",
        "with writer.as_default():\n",
        "    with tf.contrib.summary.always_record_summaries():\n",
        "        for i in range(len(my_loss_deep)):\n",
        "            tf.contrib.summary.scalar(\"loss_deep_model_with_sigmoid\", my_loss_deep[i], step=i)\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gdtigLzQY7Mg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "from io import BytesIO\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-pQDRu5pY9qo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Image('loss_deep_model_with_sigmoid.png',width=1200, height=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GuKfyJcOadae",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Tanh Activation Function**"
      ]
    },
    {
      "metadata": {
        "id": "lMtH2oByazkK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Deep_Model_tanh(tf.keras.Model): # Deep Model\n",
        "  def __init__(self):\n",
        "    super(Deep_Model_tanh, self).__init__()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    # FIX ME\n",
        "    # add some layers to your model\n",
        "    self.dense1 = tf.keras.layers.Dense(512)\n",
        "    self.drop1 = tf.keras.layers.Dropout(rate = 0.5)\n",
        "    self.dense2 = tf.keras.layers.Dense(512)\n",
        "    self.drop2 = tf.keras.layers.Dropout(rate = 0.5)\n",
        "    self.dense3 = tf.keras.layers.Dense(10)\n",
        "    \n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    # FIX ME\n",
        "    # use your layers (don't forget to add activation functions here as well\n",
        "    # if you haven't specified them in your layer definintions)\n",
        "    x = self.dense1(x)\n",
        "    x = tf.nn.sigmoid(x)\n",
        "    x = self.drop1(x)\n",
        "    x = self.dense2(x)\n",
        "    x = tf.nn.sigmoid(x)\n",
        "    x = self.drop2(x)\n",
        "    x = self.dense3(x)\n",
        "    return x # be sure to return logits, not softmax output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LZH6rBDKa9nU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "deep_model_tanh = Deep_Model_tanh()\n",
        "\n",
        "epochs = 10\n",
        "step_counter = 0\n",
        "my_loss_deep = []\n",
        "\n",
        "for epoch_n in range(epochs):\n",
        "    print('Epoch #%d' % (epoch_n))\n",
        "    for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "         loss_value = train(deep_model_tanh, images, labels)\n",
        "         my_loss_deep.append(loss_value)\n",
        "         step_counter +=1\n",
        "  \n",
        "         if step_counter % 100 == 0:\n",
        "            print('Step #%d\\tLoss: %.4f' % (step_counter, loss_value))\n",
        "\n",
        "    test_accuracy = compute_accuracy(deep_model_tanh(x_test), y_test)\n",
        "    print('Accuracy #%.2f\\n' % (test_accuracy))\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h_LelaKYbHo0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "****Visualize Loss for Deep Model with Tanh Activation Function****"
      ]
    },
    {
      "metadata": {
        "id": "86uu-HWHbE7V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "writer = tf.contrib.summary.create_file_writer(logdir='./Graph', flush_millis=1000)\n",
        "with writer.as_default():\n",
        "    with tf.contrib.summary.always_record_summaries():\n",
        "        for i in range(len(my_loss_deep)):\n",
        "            tf.contrib.summary.scalar(\"loss_deep_model_with_tanh\", my_loss_deep[i], step=i)\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dSfjYZNxbQFu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "from io import BytesIO\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gx_veeUidsAh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Image('loss_deep_model_with_tanh.png',width=1200, height=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OVu_4jiB9D-M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### c. Different gradient descent optimizers"
      ]
    },
    {
      "metadata": {
        "id": "z6kYpYWqiAng",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**AdamOptimizer**"
      ]
    },
    {
      "metadata": {
        "id": "y-5UsUFx9Jk7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Adamoptimizer = tf.train.AdamOptimizer(learning_rate=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FXLiqIyqijc8"
      },
      "cell_type": "code",
      "source": [
        "def train(model, images, labels):\n",
        "  # You should not need to modify this function\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(images)\n",
        "    loss_value = loss(logits, labels)  \n",
        "  grads = tape.gradient(loss_value, model.variables)\n",
        "  Adamoptimizer.apply_gradients(zip(grads, model.variables))\n",
        "  return loss_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MsIyHTNyiXFz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Deep_Model(tf.keras.Model): # Deep Model\n",
        "  def __init__(self):\n",
        "    super(Deep_Model, self).__init__()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    # FIX ME\n",
        "    # add some layers to your model\n",
        "    self.dense1 = tf.keras.layers.Dense(512)\n",
        "    self.drop1 = tf.keras.layers.Dropout(rate = 0.5)\n",
        "    self.dense2 = tf.keras.layers.Dense(512)\n",
        "    self.drop2 = tf.keras.layers.Dropout(rate = 0.5)\n",
        "    self.dense3 = tf.keras.layers.Dense(10)\n",
        "    \n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    # FIX ME\n",
        "    # use your layers (don't forget to add activation functions here as well\n",
        "    # if you haven't specified them in your layer definintions)\n",
        "    x = self.dense1(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.drop1(x)\n",
        "    x = self.dense2(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.drop2(x)\n",
        "    x = self.dense3(x)\n",
        "    return x # be sure to return logits, not softmax output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-tXPip6DiZpF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "deep_model = Deep_Model()\n",
        "\n",
        "epochs = 10\n",
        "step_counter = 0\n",
        "my_loss_deep = []\n",
        "\n",
        "for epoch_n in range(epochs):\n",
        "    print('Epoch #%d' % (epoch_n))\n",
        "    for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "         loss_value = train(deep_model, images, labels)\n",
        "         my_loss_deep.append(loss_value)\n",
        "         step_counter +=1\n",
        "  \n",
        "         if step_counter % 100 == 0:\n",
        "            print('Step #%d\\tLoss: %.4f' % (step_counter, loss_value))\n",
        "\n",
        "    test_accuracy = compute_accuracy(deep_model(x_test), y_test)\n",
        "    print('Accuracy #%.2f\\n' % (test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-S9Vm-2GineM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Visualizing Loss for Deep Model with AdamOptimizer**"
      ]
    },
    {
      "metadata": {
        "id": "2haxd6PIim_-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "writer = tf.contrib.summary.create_file_writer(logdir='./Graph', flush_millis=1000)\n",
        "with writer.as_default():\n",
        "    with tf.contrib.summary.always_record_summaries():\n",
        "        for i in range(len(my_loss_deep)):\n",
        "            tf.contrib.summary.scalar(\"loss_deep_model_with_AdamOptimizer\", my_loss_deep[i], step=i)\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OqWKTh3RjFTg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "from io import BytesIO\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FFlkcKr5jl5D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Image('loss_deep_model_with_adamoptimizer.png',width=1200, height=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GJGz-ZmElqJR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**AdagradOptimizer**"
      ]
    },
    {
      "metadata": {
        "id": "SUPNKH9pl6VH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "AdagradOptimizer = tf.train.AdagradOptimizer(learning_rate=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qcXIppiBmAJz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, images, labels):\n",
        "  # You should not need to modify this function\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(images)\n",
        "    loss_value = loss(logits, labels)  \n",
        "  grads = tape.gradient(loss_value, model.variables)\n",
        "  AdagradOptimizer.apply_gradients(zip(grads, model.variables))\n",
        "  return loss_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YNKjRs3wmElE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Deep_Model(tf.keras.Model): # Deep Model\n",
        "  def __init__(self):\n",
        "    super(Deep_Model, self).__init__()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    # FIX ME\n",
        "    # add some layers to your model\n",
        "    self.dense1 = tf.keras.layers.Dense(512)\n",
        "    self.drop1 = tf.keras.layers.Dropout(rate = 0.5)\n",
        "    self.dense2 = tf.keras.layers.Dense(512)\n",
        "    self.drop2 = tf.keras.layers.Dropout(rate = 0.5)\n",
        "    self.dense3 = tf.keras.layers.Dense(10)\n",
        "    \n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    # FIX ME\n",
        "    # use your layers (don't forget to add activation functions here as well\n",
        "    # if you haven't specified them in your layer definintions)\n",
        "    x = self.dense1(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.drop1(x)\n",
        "    x = self.dense2(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.drop2(x)\n",
        "    x = self.dense3(x)\n",
        "    return x # be sure to return logits, not softmax output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oej2alyImJFN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "deep_model = Deep_Model()\n",
        "\n",
        "epochs = 10\n",
        "step_counter = 0\n",
        "my_loss_deep = []\n",
        "\n",
        "for epoch_n in range(epochs):\n",
        "    print('Epoch #%d' % (epoch_n))\n",
        "    for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "         loss_value = train(deep_model, images, labels)\n",
        "         my_loss_deep.append(loss_value)\n",
        "         step_counter +=1\n",
        "  \n",
        "         if step_counter % 100 == 0:\n",
        "            print('Step #%d\\tLoss: %.4f' % (step_counter, loss_value))\n",
        "\n",
        "    test_accuracy = compute_accuracy(deep_model(x_test), y_test)\n",
        "    print('Accuracy #%.2f\\n' % (test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0rFK_R8mmOHP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Visualizing Loss for Deep Model with MomentumOptimizer**"
      ]
    },
    {
      "metadata": {
        "id": "HoQ36N11mRKU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "writer = tf.contrib.summary.create_file_writer(logdir='./Graph', flush_millis=1000)\n",
        "with writer.as_default():\n",
        "    with tf.contrib.summary.always_record_summaries():\n",
        "        for i in range(len(my_loss_deep)):\n",
        "            tf.contrib.summary.scalar(\"loss_deep_model_with_AdagradOptimizer\", my_loss_deep[i], step=i)\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V11ievsImW6B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "from io import BytesIO\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u6Qirl8tmXqR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Image('loss_deep_model_with_AdagradOptimizer.png',width=1200, height=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mzcDnHGVEoGM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### d. Different weight initialization strategies"
      ]
    },
    {
      "metadata": {
        "id": "XsW3-_eYtBEo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**With Zeros Initializer**"
      ]
    },
    {
      "metadata": {
        "id": "poY_LwvTErSS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Deep_Model(tf.keras.Model): # Deep Model\n",
        "  def __init__(self):\n",
        "    super(Deep_Model, self).__init__()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    # FIX ME\n",
        "    # add some layers to your model\n",
        "    self.dense1 = tf.keras.layers.Dense(512, kernel_initializer = \"Zeros\")\n",
        "    self.drop1 = tf.keras.layers.Dropout(rate = 0.5)\n",
        "    self.dense2 = tf.keras.layers.Dense(512, kernel_initializer = \"Zeros\")\n",
        "    self.drop2 = tf.keras.layers.Dropout(rate = 0.5)\n",
        "    self.dense3 = tf.keras.layers.Dense(10)\n",
        "    \n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    # FIX ME\n",
        "    # use your layers (don't forget to add activation functions here as well\n",
        "    # if you haven't specified them in your layer definintions)\n",
        "    x = self.dense1(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.drop1(x)\n",
        "    x = self.dense2(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.drop2(x)\n",
        "    x = self.dense3(x)\n",
        "    return x # be sure to return logits, not softmax output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1EM0ZXG9uaUJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "deep_model = Deep_Model()\n",
        "\n",
        "epochs = 10\n",
        "step_counter = 0\n",
        "my_loss_deep = []\n",
        "\n",
        "for epoch_n in range(epochs):\n",
        "    print('Epoch #%d' % (epoch_n))\n",
        "    for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "         loss_value = train(deep_model, images, labels)\n",
        "         my_loss_deep.append(loss_value)\n",
        "         step_counter +=1\n",
        "  \n",
        "         if step_counter % 100 == 0:\n",
        "            print('Step #%d\\tLoss: %.4f' % (step_counter, loss_value))\n",
        "\n",
        "    test_accuracy = compute_accuracy(deep_model(x_test), y_test)\n",
        "    print('Accuracy #%.2f\\n' % (test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Y_QPGOHuj-Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Visualizing Loss for Deep Model with kernel.initialier.Zeros**"
      ]
    },
    {
      "metadata": {
        "id": "glzfDOp4uo6l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "writer = tf.contrib.summary.create_file_writer(logdir='./Graph', flush_millis=1000)\n",
        "with writer.as_default():\n",
        "    with tf.contrib.summary.always_record_summaries():\n",
        "        for i in range(len(my_loss_deep)):\n",
        "            tf.contrib.summary.scalar(\"loss_deep_model_with_kernel.initialier.Zeros\", my_loss_deep[i], step=i)\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S-gHh7HDuuyv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "from io import BytesIO\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "neeDHezTwm_r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Image('loss_deep_model_with_kernel.initializer.Zeros.png',width=1200, height=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qlSDV_9yx59Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**RandomNormal Initializer**"
      ]
    },
    {
      "metadata": {
        "id": "xshLyxLbyAp3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Deep_Model(tf.keras.Model): # Deep Model\n",
        "  def __init__(self):\n",
        "    super(Deep_Model, self).__init__()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    # FIX ME\n",
        "    # add some layers to your model\n",
        "    self.dense1 = tf.keras.layers.Dense(512, kernel_initializer = \"RandomNormal\")\n",
        "    self.drop1 = tf.keras.layers.Dropout(rate = 0.5)\n",
        "    self.dense2 = tf.keras.layers.Dense(512, kernel_initializer = \"RandomNormal\")\n",
        "    self.drop2 = tf.keras.layers.Dropout(rate = 0.5)\n",
        "    self.dense3 = tf.keras.layers.Dense(10)\n",
        "    \n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    # FIX ME\n",
        "    # use your layers (don't forget to add activation functions here as well\n",
        "    # if you haven't specified them in your layer definintions)\n",
        "    x = self.dense1(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.drop1(x)\n",
        "    x = self.dense2(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.drop2(x)\n",
        "    x = self.dense3(x)\n",
        "    return x # be sure to return logits, not softmax output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JTu6FduRyH1m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "deep_model = Deep_Model()\n",
        "\n",
        "epochs = 10\n",
        "step_counter = 0\n",
        "my_loss_deep = []\n",
        "\n",
        "for epoch_n in range(epochs):\n",
        "    print('Epoch #%d' % (epoch_n))\n",
        "    for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "         loss_value = train(deep_model, images, labels)\n",
        "         my_loss_deep.append(loss_value)\n",
        "         step_counter +=1\n",
        "  \n",
        "         if step_counter % 100 == 0:\n",
        "            print('Step #%d\\tLoss: %.4f' % (step_counter, loss_value))\n",
        "\n",
        "    test_accuracy = compute_accuracy(deep_model(x_test), y_test)\n",
        "    print('Accuracy #%.2f\\n' % (test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W_HSGnGeySjS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Visualizing Loss for Deep Model with kernel.initialier.RandomNormal**"
      ]
    },
    {
      "metadata": {
        "id": "i0Ian5puyT_8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "writer = tf.contrib.summary.create_file_writer(logdir='./Graph', flush_millis=1000)\n",
        "with writer.as_default():\n",
        "    with tf.contrib.summary.always_record_summaries():\n",
        "        for i in range(len(my_loss_deep)):\n",
        "            tf.contrib.summary.scalar(\"loss_deep_model_with_kernel.initialier.RandomNormal\", my_loss_deep[i], step=i)\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YvdUZQ-s0dRW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "from io import BytesIO\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8PeX4eayXCS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Image('loss_deep_model_with_kernel.initializer.RandomNormal.png',width=1200, height=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CznPh4YX1Lyv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Write Up"
      ]
    },
    {
      "metadata": {
        "id": "77Nf-wP-17iX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   With a high learning rate 0.5, accuracy has been very high accuracy 0.98. For low learning rate 0.01, the accuracy can also reach 0.98. However, with a reasonable learning rate, the accuracy is only about 0.92. It does not imply that the higher or the lower the learning rate, the better the accuracy.\n",
        "\n",
        "*   The accuracy for using Sigmoind and Tanh activation functions are about the same, which is around 0.93. No obvious difference for the activation functions I chose for this case. \n",
        "\n",
        "*  The accuracy for choosing AdamOptimizer is only 0.38, which is very low and should not be adapted in this model, while the AdaradOptimizer has a high accuracy of 0.98.\n",
        "\n",
        "*   By initializing weights to be Zeros, the accuracy is super low (0.11), while using RandomNormal weights initialization, the accuracy can reach 0.98. In this case, we would definitely choose RandomNormal weights initialization strategy. \n",
        "\n"
      ]
    }
  ]
}